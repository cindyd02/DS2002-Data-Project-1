{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data File Ingestion Summary ---\n",
      "Summary:\n",
      "Number of records: 699\n",
      "Number of columns: 10\n",
      "Column names:\n",
      "- match\n",
      "- teamInnings\n",
      "- battingPos\n",
      "- batsmanName\n",
      "- dismissal\n",
      "- runs\n",
      "- balls\n",
      "- 4s\n",
      "- 6s\n",
      "- SR\n",
      "\n",
      "--- Column Modification ---\n",
      "Added column 'wins' with value '10'\n",
      "Modified data saved to 'output.csv'\n",
      "\n",
      "--- Post Processing Summary ---\n",
      "Summary:\n",
      "Number of records: 699\n",
      "Number of columns: 11\n",
      "Column names:\n",
      "- match\n",
      "- teamInnings\n",
      "- battingPos\n",
      "- batsmanName\n",
      "- dismissal\n",
      "- runs\n",
      "- balls\n",
      "- 4s\n",
      "- 6s\n",
      "- SR\n",
      "- wins\n",
      "\n",
      "--- Summary Comparison ---\n",
      "Input:  699 records, 10 columns\n",
      "Output: 699 records, 11 columns\n",
      "Modified data saved to 'output.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import sqlite3\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Function to load data based on file type (JSON or CSV)\n",
    "def load_data(file_path, file_type):\n",
    "    \"\"\"\n",
    "    Load data from a specified file.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: Path to the input file.\n",
    "    - file_type: Type of the file ('csv' or 'json').\n",
    "    \n",
    "    Returns:\n",
    "    - Loaded data as a pandas DataFrame (for CSV) or dictionary (for JSON).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_type == 'json':\n",
    "            with open(file_path, 'r') as file:\n",
    "                return json.load(file)\n",
    "        elif file_type == 'csv':\n",
    "            return pd.read_csv(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to convert a JSON file into a CSV file\n",
    "def json_to_csv(json_data, output_path):\n",
    "    \"\"\"\n",
    "    Convert JSON data to CSV and save it to a file.\n",
    "    \n",
    "    Parameters:\n",
    "    - json_data: JSON data to be converted.\n",
    "    - output_path: Path to save the converted CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.json_normalize(json_data, 'battingSummary')\n",
    "        data.to_csv(output_path, index=False)\n",
    "        print(f\"Converted JSON to CSV at {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting JSON to CSV: {e}\")\n",
    "\n",
    "# Function to convert a CSV file into a JSON file\n",
    "def csv_to_json(csv_data, output_path):\n",
    "    \"\"\"\n",
    "    Convert a CSV DataFrame to JSON and save it to a file.\n",
    "    \n",
    "    Parameters:\n",
    "    - csv_data: Pandas DataFrame containing CSV data.\n",
    "    - output_path: Path to save the converted JSON file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        csv_data.to_json(output_path, orient='records', indent=4)\n",
    "        print(f\"Converted CSV to JSON at {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting CSV to JSON: {e}\")\n",
    "\n",
    "# Function to save data to an SQLite database\n",
    "def save_to_db(data, table_name, db_name='etl_data.db'):\n",
    "    \"\"\"\n",
    "    Save pandas DataFrame to a SQLite database.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Pandas DataFrame to be saved.\n",
    "    - table_name: Name of the table in the SQLite database.\n",
    "    - db_name: Name of the SQLite database file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        data.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "        print(f\"Saved data to table '{table_name}' in {db_name}\")\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to database: {e}\")\n",
    "\n",
    "# Function to generate a summary of the data (records and columns)\n",
    "def generate_summary(data):\n",
    "    \"\"\"\n",
    "    Generate and print a summary of the DataFrame, including number of records and columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Pandas DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple (number of records, number of columns).\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        num_records = len(data)\n",
    "        num_columns = len(data.columns)\n",
    "        print(f\"Summary:\")\n",
    "        print(f\"Number of records: {num_records}\")\n",
    "        print(f\"Number of columns: {num_columns}\")\n",
    "        print(\"Column names:\")\n",
    "        for col in data.columns:\n",
    "            print(f\"- {col}\")\n",
    "        return num_records, num_columns\n",
    "    else:\n",
    "        print(\"No summary available for this data format.\")\n",
    "        return 0, 0\n",
    "\n",
    "# Function to display the first 5 rows of a SQLite table\n",
    "def display_sqlite_table(db_name, table_name):\n",
    "    \"\"\"\n",
    "    Display the first 5 rows of a specified table from a SQLite database.\n",
    "    \n",
    "    Parameters:\n",
    "    - db_name: Name of the SQLite database.\n",
    "    - table_name: Name of the table to display.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"SELECT * FROM {table_name} LIMIT 5\")\n",
    "        rows = cursor.fetchall()\n",
    "        headers = [description[0] for description in cursor.description]\n",
    "        print(f\"\\nFirst 5 rows of table '{table_name}':\")\n",
    "        print(tabulate(rows, headers=headers, tablefmt=\"grid\"))\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying SQLite table: {e}\")\n",
    "\n",
    "# Function to interactively add or delete columns from the dataset\n",
    "def interactive_column_modification(data):\n",
    "    \"\"\"\n",
    "    Interactive function to add or delete columns in a DataFrame. User can manually add specific data and delete columns until the 'done' option is inputted and the DataFrame is returned. \n",
    "    \n",
    "    Parameters:\n",
    "    - data: Pandas DataFrame to modify.\n",
    "    \n",
    "    Returns:\n",
    "    - Modified DataFrame.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        action = input(\"Do you want to add or delete a column? (add/delete/done): \").lower()\n",
    "        \n",
    "        if action == 'add':\n",
    "            col_name = input(\"Enter the name of the column to add: \")\n",
    "            col_value = input(\"Enter the value for the new column: \")\n",
    "            data[col_name] = col_value\n",
    "            print(f\"Added column '{col_name}' with value '{col_value}'\")\n",
    "        \n",
    "        elif action == 'delete':\n",
    "            col_name = input(\"Enter the name of the column to delete: \")\n",
    "            if col_name in data.columns:\n",
    "                data = data.drop(columns=[col_name])\n",
    "                print(f\"Deleted column '{col_name}'\")\n",
    "            else:\n",
    "                print(f\"Column '{col_name}' not found in the dataset\")\n",
    "        \n",
    "        elif action == 'done':\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"Invalid action. Please enter 'add', 'delete', or 'done'.\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Function to convert JSON to a pandas DataFrame\n",
    "def json_to_dataframe(json_data):\n",
    "    \"\"\"\n",
    "    Convert JSON data to a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - json_data: JSON data to be converted.\n",
    "    \n",
    "    Returns:\n",
    "    - Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    return pd.json_normalize(json_data, 'battingSummary')\n",
    "\n",
    "# Function to convert a DataFrame back to JSON format\n",
    "def dataframe_to_json(df):\n",
    "    \"\"\"\n",
    "    Convert a pandas DataFrame to JSON.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Pandas DataFrame to be converted.\n",
    "    \n",
    "    Returns:\n",
    "    - JSON object.\n",
    "    \"\"\"\n",
    "    return json.loads(df.to_json(orient='records'))\n",
    "\n",
    "# Main ETL pipeline function to load, process, modify, and save data\n",
    "def etl_pipeline(input_file, input_type, output_type, modify=True):\n",
    "    \"\"\"\n",
    "    End-to-end ETL pipeline to load data, process, modify (if needed), and save to the desired output format.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_file: Input file path.\n",
    "    - input_type: Type of the input file ('csv' or 'json').\n",
    "    - output_type: Desired output format ('csv', 'json', or 'sql').\n",
    "    - modify: Boolean indicating if interactive column modifications should be allowed.\n",
    "    \n",
    "    Returns:\n",
    "    - Processed data in the desired format.\n",
    "    \"\"\"\n",
    "    data = load_data(input_file, input_type)\n",
    "    if data is None:\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Data File Ingestion Summary ---\")\n",
    "    if input_type == 'json':\n",
    "        data = json_to_dataframe(data)\n",
    "    input_records, input_columns = generate_summary(data)\n",
    "\n",
    "    if modify:\n",
    "        print(\"\\n--- Column Modification ---\")\n",
    "        data = interactive_column_modification(data)\n",
    "\n",
    "    if output_type == 'csv':\n",
    "        output_data = data\n",
    "        output_data.to_csv('output.csv', index=False)\n",
    "        print(\"Modified data saved to 'output.csv'\")\n",
    "    elif output_type == 'json':\n",
    "        output_data = dataframe_to_json(data)\n",
    "        with open('output.json', 'w') as f:\n",
    "            json.dump(output_data, f, indent=4)\n",
    "        print(\"Modified data saved to 'output.json'\")\n",
    "    elif output_type == 'sql':\n",
    "        output_data = data\n",
    "        table_name = os.path.splitext(os.path.basename(input_file))[0]\n",
    "        save_to_db(output_data, table_name)\n",
    "        display_sqlite_table('etl_data.db', table_name)\n",
    "\n",
    "    print(\"\\n--- Post Processing Summary ---\")\n",
    "    output_records, output_columns = generate_summary(data)\n",
    "\n",
    "    print(\"\\n--- Summary Comparison ---\")\n",
    "    print(f\"Input:  {input_records} records, {input_columns} columns\")\n",
    "    print(f\"Output: {output_records} records, {output_columns} columns\")\n",
    "\n",
    "    return output_data\n",
    "\n",
    "# Main function to run the ETL pipeline\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the ETL pipeline. It prompts the user for input file type, output format, and runs the pipeline.\n",
    "    \"\"\"\n",
    "    input_file = input(\"Enter the input file name: \")\n",
    "    input_type = input(\"Enter the input file type (csv/json): \").lower()\n",
    "    output_type = input(\"Enter the desired output type (csv/json/sql): \").lower()\n",
    "\n",
    "    modified_data = etl_pipeline(input_file, input_type, output_type)\n",
    "\n",
    "    if output_type == 'csv' and isinstance(modified_data, pd.DataFrame):\n",
    "        modified_data.to_csv('output.csv', index=False)\n",
    "        print(\"Modified data saved to 'output.csv'\")\n",
    "    elif output_type == 'json' and isinstance(modified_data, pd.DataFrame):\n",
    "        modified_data.to_json('output.json', orient='records', indent=4)\n",
    "        print(\"Modified data saved to 'output.json'\")\n",
    "\n",
    "# Execute the main function when the script is run\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
